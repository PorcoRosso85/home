# RAG System Development Plan

## 概要

高品質なRAG（Retrieval-Augmented Generation）システムの構築計画。KuzuDBを中心に、Rustの高速処理とPythonの機械学習エコシステムを組み合わせた実装を行う。

## システムアーキテクチャ

```
文書入力 → 抽出 → チャンク分割 → セマンティック強化 → グラフ構築 → 
KuzuDB (VSS/FTS/Graph) → ハイブリッド検索 → 再ランキング → 回答生成
```

## 開発フェーズ

### Phase 1: KuzuDB基盤構築 (1-2週間)

**目的**: KuzuDBに全てを統合した基盤を構築

**実装内容**:
- KuzuDBセットアップ（VSS, FTS拡張）
- 基本スキーマ定義（ノード、エッジ、ベクトル）
- 文書取り込み（extractous/rs → KuzuDB直接格納）
- シンプルなチャンク分割（text-splitter/rs）
- 基本的な全文検索・ベクトル検索

**KuzuDBが担当**:
- データストレージ
- グラフ構造管理
- ベクトル検索（VSS）
- 全文検索（FTS）
- 基本的なクエリ処理

### Phase 2: セマンティックグラフ生成 (2-3週間)

**目的**: contextual chunking graph/pyで高品質なグラフを構築

**実装内容**:
- contextual chunking実装
- LLMによる概念抽出
- チャンク間の関係性計算
- KuzuDBへのグラフ構造格納
- 埋め込み生成とVSSへの格納

**ccg/pyが担当**:
- 文脈強化チャンク生成
- セマンティック関係抽出
- グラフ構造の設計

### Phase 3: ハイブリッド検索実装 (2週間)

**目的**: 高精度な検索システム構築

**実装内容**:
- VSS + FTS + グラフ探索の統合
- Reciprocal Rank Fusion（aichat参考）
- Cohere Rerankの統合
- 回答完全性チェック機能

**KuzuDB内で完結**:
- ハイブリッドクエリ実行
- グラフトラバース
- 結果の統合

### Phase 4: 最適化・評価 (1-2週間)

**目的**: パフォーマンスと精度の改善

**実装内容**:
- クエリキャッシュ実装
- バッチ処理最適化
- 評価メトリクス実装
- A/Bテスト環境構築

**追加最適化**:
- HNSW実装検討（aichat参考）
- Rust高速化部分の拡張

### Phase 5: プロダクション対応 (1週間)

**目的**: 本番環境への対応

**実装内容**:
- API層構築
- モニタリング実装
- エラーハンドリング
- スケーリング対応

## ライブラリ候補一覧

### 文書抽出層

#### extractous/rs
- **用途**: 汎用文書抽出（PDF, DOCX, PPTX等）
- **検討理由**: Rust実装で高速、多様なフォーマット対応、メモリ効率的な大容量文書処理

#### calamine/rs
- **用途**: Excel特化処理
- **検討理由**: 複雑な表計算、数式、マクロの処理が可能、セル間の関係性保持

#### LlamaParse
- **用途**: 高品質PDF解析
- **検討理由**: 構造保持したマークダウン変換、contextual chunking graphで実績あり

### テキスト処理層

#### text-splitter/rs
- **用途**: 高速テキスト分割
- **検討理由**: Rust実装で高速、カスタマイズ可能な分割戦略、メモリセーフ

#### RecursiveCharacterTextSplitter (LangChain)
- **用途**: インテリジェントなテキスト分割
- **検討理由**: オーバーラップ付き分割、意味的境界の保持、contextual chunkingで実績

### 埋め込み生成層

#### sentence-transformers
- **用途**: 文章埋め込み生成
- **検討理由**: 5000+の事前学習モデル、研究・開発で広く使用、高品質な埋め込み

#### FastEmbed
- **用途**: 軽量埋め込み生成
- **検討理由**: ONNX実装で高速、最小限の依存関係、サーバーレス環境に最適

#### Candle
- **用途**: Rust純正ML処理
- **検討理由**: Pythonオーバーヘッドなし、サーバーレス対応、軽量バイナリ

#### EmbedAnything
- **用途**: マルチモーダル埋め込み
- **検討理由**: 複数ファイル形式対応、ストリーミング処理、本番環境実績

#### OpenAI Embeddings API
- **用途**: 高品質埋め込み生成
- **検討理由**: 最高品質の埋め込み、API利用で実装簡単、contextual chunkingで使用実績

### 類似度計算層

#### SimSIMD
- **用途**: 高速ベクトル類似度計算
- **検討理由**: SIMD最適化で最大200倍高速、多言語対応、最小メモリ使用量

#### FAISS
- **用途**: 大規模ベクトル検索
- **検討理由**: Facebook開発、業界標準、contextual chunking graphで実績

#### HNSW (from aichat)
- **用途**: 効率的な近似最近傍探索
- **検討理由**: 高速検索、メモリ効率的、aichatで実装済み

### データベース層

#### KuzuDB
- **用途**: グラフデータベース
- **検討理由**: 
  - グラフ構造とベクトル検索の統合
  - VSS拡張でベクトル検索対応
  - FTS拡張で全文検索対応
  - ACID準拠
  - 効率的なグラフトラバース

#### NetworkX
- **用途**: グラフ構造管理
- **検討理由**: Pythonでのグラフアルゴリズム実装、contextual chunkingで使用

### 検索最適化層

#### BM25 (rank_bm25)
- **用途**: キーワード検索
- **検討理由**: 古典的だが効果的、完全一致検索に必須、ハイブリッド検索の基盤

#### Cohere Rerank
- **用途**: 検索結果の再ランキング
- **検討理由**: クロスエンコーダーで高精度、API利用で簡単、contextual chunkingで実績

### LLM/AI層

#### Anthropic Claude
- **用途**: 文脈強化、概念抽出
- **検討理由**: 高品質な文章理解、プロンプトキャッシング対応、コスト効率的

#### OpenAI GPT-4
- **用途**: 概念抽出、回答生成
- **検討理由**: 最高品質の言語理解、contextual chunkingで実績、豊富なAPI機能

### ユーティリティ

#### LangChain
- **用途**: LLMパイプライン構築
- **検討理由**: 豊富な抽象化、多数のインテグレーション、開発効率向上

#### Pydantic
- **用途**: データ検証
- **検討理由**: 型安全性、自動バリデーション、構造化データ管理

## POCプロジェクトからの参照

### similarity-ts
- AST解析の並列処理 → 文書処理の高速化
- ブルームフィルタ → ベクトル検索の前処理最適化
- メモリ効率的アルゴリズム → 大規模文書処理

### dokosa
- スライディングウィンドウ分割 → text-splitter/rsの実装参考
- シンプルなベクトルインデックス → KuzuDB VSS実装の基礎
- Git連携による差分更新 → 文書の増分インデックス化

### aichat
- HNSW実装 → 高速ベクトル検索
- BM25統合 → ハイブリッド検索の実装例
- Reciprocal Rank Fusion → 検索結果統合アルゴリズム
- 言語別チャンク分割 → contextual chunkingの基礎

### contextual_chunking_graph
- 文脈強化ロジック → そのまま流用可能
- グラフ構築・探索 → KuzuDBでの実装参考
- 回答完全性チェック → クエリ処理層に統合
- 評価フレームワーク → システム評価に活用

## 選定基準

1. **パフォーマンス**: Rust実装を優先し、処理速度を最大化
2. **統合性**: KuzuDBとの親和性、既存POCからの流用可能性
3. **実績**: contextual chunking graphやaichatでの使用実績
4. **保守性**: アクティブなメンテナンス、ドキュメントの充実度
5. **スケーラビリティ**: 大規模データセットへの対応能力

## 次のステップ

1. Phase 1の実装開始（KuzuDB基盤構築）
2. 各ライブラリのベンチマーク実施
3. プロトタイプによる検証
4. 最終的なライブラリ選定