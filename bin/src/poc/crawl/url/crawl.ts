#!/usr/bin/env -S deno run --allow-net

/**
 * URL Crawler - Extract links from websites
 * Inspired by sitefetch's crawling logic
 */

import {
  CONTENT_TYPE_HTML,
  DEFAULT_CONCURRENCY,
  DEFAULT_DEPTH,
  DEFAULT_SAME_HOST,
  FORMAT_JSON,
  FORMAT_TEXT,
} from "./variables/constants.ts";

export interface CrawlOptions {
  concurrency?: number;
  match?: string[];
  limit?: number;
  sameHost?: boolean;
  depth?: number;
  timeout?: number;
  headers?: Record<string, string>;
  retries?: number;
  retryDelay?: number;
  onProgress?: (completed: number, total: number) => void;
  withRoot?: boolean;
}

export interface CrawlResult {
  url: string;
  links: string[];
  error?: string;
  depth?: number;
}

export class URLCrawler {
  private visited = new Set<string>();
  private queue: string[] = [];
  private results: CrawlResult[] = [];
  private baseHost: string;
  private basePath: string;
  public options: CrawlOptions;

  constructor(
    private baseUrl: string,
    options: CrawlOptions = {},
  ) {
    const url = new URL(baseUrl);
    this.baseHost = url.host;
    
    // ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã®æ±ºå®š
    if (url.pathname.endsWith('/')) {
      // æ—¢ã«æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹
      this.basePath = url.pathname;
    } else if (url.pathname.match(/\.[^/]+$/)) {
      // ãƒ•ã‚¡ã‚¤ãƒ«ã£ã½ã„URLï¼ˆæ‹¡å¼µå­ãŒã‚ã‚‹ï¼‰
      // ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã‚’æŠ½å‡º
      const lastSlash = url.pathname.lastIndexOf('/');
      this.basePath = url.pathname.substring(0, lastSlash + 1);
    } else {
      // ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦æ‰±ã†ï¼ˆæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ ï¼‰
      this.basePath = url.pathname + '/';
    }
    
    this.options = {
      concurrency: DEFAULT_CONCURRENCY,
      sameHost: DEFAULT_SAME_HOST,
      depth: DEFAULT_DEPTH,
      match: [],
      ...options,
    };
  }

  async crawl(): Promise<CrawlResult[]> {
    this.queue.push(this.baseUrl);

    while (this.queue.length > 0 && this.shouldContinue()) {
      const batch = this.queue.splice(0, this.options.concurrency!);
      const promises = batch.map((url) => this.processUrl(url));
      await Promise.all(promises);
    }

    return this.results;
  }

  private shouldContinue(): boolean {
    if (this.options.limit && this.visited.size >= this.options.limit) {
      return false;
    }
    return true;
  }

  private async processUrl(url: string): Promise<void> {
    if (this.visited.has(url)) return;
    this.visited.add(url);

    // æœ€åˆã®URLï¼ˆbaseUrlï¼‰ã¯å¸¸ã«å‡¦ç†ã™ã‚‹ã€ãã‚Œä»¥å¤–ã¯ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚’é©ç”¨
    if (url !== this.baseUrl && !this.matchesPattern(url)) return;

    try {
      const response = await fetch(url, {
        redirect: "manual"  // ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’æ‰‹å‹•ã§å‡¦ç†
      });
      
      // ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®å‡¦ç†
      if (response.status >= 300 && response.status < 400) {
        const location = response.headers.get("location");
        if (location) {
          const redirectUrl = new URL(location, url);
          // ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
          if (!this.visited.has(redirectUrl.href) && this.shouldCrawlLink(redirectUrl.href)) {
            this.queue.push(redirectUrl.href);
          }
        }
        await response.body?.cancel();
        return;
      }
      
      if (
        !response.ok ||
        !response.headers.get("content-type")?.includes(CONTENT_TYPE_HTML)
      ) {
        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã‚’æ¶ˆè²»ã—ã¦ãƒªãƒ¼ã‚¯ã‚’é˜²ã
        await response.body?.cancel();
        return;
      }

      const html = await response.text();
      const links = this.extractLinks(html, url);

      this.results.push({ url, links });

      // Add new links to queue
      for (const link of links) {
        if (!this.visited.has(link) && this.shouldCrawlLink(link)) {
          this.queue.push(link);
        }
      }
    } catch (error) {
      this.results.push({
        url,
        links: [],
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  private extractLinks(html: string, baseUrl: string): string[] {
    const linkRegex = /<a[^>]+href=["']([^"']+)["'][^>]*>/gi;
    const links: string[] = [];
    let match;

    while ((match = linkRegex.exec(html)) !== null) {
      try {
        const href = match[1];
        const absoluteUrl = new URL(href, baseUrl);

        // httpã¾ãŸã¯httpsã®URLã®ã¿ã‚’å—ã‘å…¥ã‚Œã‚‹
        if (
          absoluteUrl.protocol === "http:" || absoluteUrl.protocol === "https:"
        ) {
          // ãƒãƒƒã‚·ãƒ¥ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ã—ã¦æ­£è¦åŒ–ï¼ˆsitefetchäº’æ›ï¼‰
          absoluteUrl.hash = '';
          links.push(absoluteUrl.href);
        }
      } catch {
        // Ignore invalid URLs
      }
    }

    return [...new Set(links)]; // Remove duplicates
  }

  private shouldCrawlLink(url: string): boolean {
    try {
      const linkUrl = new URL(url);

      // Check same host restriction
      if (this.options.sameHost && linkUrl.host !== this.baseHost) {
        return false;
      }

      // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã®ã¿ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆwithRootãŒç„¡åŠ¹ã®å ´åˆï¼‰
      if (!this.options.withRoot) {
        if (!this.options.match || this.options.match.length === 0) {
          // ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ï¼ˆ"/"ï¼‰ã®å ´åˆã¯å…¨ä½“ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½
          if (this.basePath === '/') {
            return true;
          }
          // ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹é…ä¸‹ã®ã¿è¨±å¯
          if (!linkUrl.pathname.startsWith(this.basePath)) {
            return false;
          }
        }
      }

      // Check match patterns
      if (!this.matchesPattern(url)) {
        return false;
      }

      return true;
    } catch {
      return false;
    }
  }

  private matchesPattern(url: string): boolean {
    if (!this.options.match || this.options.match.length === 0) {
      return true;
    }

    const urlPath = new URL(url).pathname;
    
    let included = false;
    let excluded = false;
    
    for (const pattern of this.options.match) {
      // å¦å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡¦ç†
      if (pattern.startsWith('!')) {
        const negPattern = pattern.slice(1);
        const regex = this.globToRegex(negPattern);
        if (regex.test(urlPath)) {
          excluded = true;
        }
      } else {
        // é€šå¸¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã®å‡¦ç†ã‚’å«ã‚€ï¼‰
        let effectivePattern = pattern;
        
        // ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã€ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’å‰ã«ä»˜ã‘ã‚‹
        if (!pattern.startsWith('/')) {
          effectivePattern = this.basePath + pattern;
        }
        
        const regex = this.globToRegex(effectivePattern);
        if (regex.test(urlPath)) {
          included = true;
        }
      }
    }
    
    // å¦å®šãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ€å„ªå…ˆ
    if (excluded) return false;
    
    // é€šå¸¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°ã€ãã‚Œã«ãƒãƒƒãƒã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    const hasIncludePatterns = this.options.match.some(p => !p.startsWith('!'));
    if (hasIncludePatterns) {
      return included;
    }
    
    // å¦å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã®å ´åˆã€é™¤å¤–ã•ã‚Œã¦ã„ãªã‘ã‚Œã°OK
    return true;
  }

  private globToRegex(glob: string): RegExp {
    // ã¾ãšç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ï¼ˆãŸã ã— * ã¨ ? ã¯é™¤ãï¼‰
    const escaped = glob.replace(/[.+^${}()|[\]\\]/g, "\\$&");
    
    // globãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£è¦è¡¨ç¾ã«å¤‰æ›
    const pattern = escaped
      .replace(/\*\*/g, "@@DOUBLESTAR@@")  // ** ã‚’ä¸€æ™‚çš„ã«ç½®æ›
      .replace(/\*/g, "[^/]*")              // * ã¯ / ä»¥å¤–ã®ä»»æ„ã®æ–‡å­—
      .replace(/\?/g, "[^/]")               // ? ã¯ / ä»¥å¤–ã®1æ–‡å­—
      .replace(/@@DOUBLESTAR@@/g, ".*");   // ** ã¯ä»»æ„ã®æ–‡å­—ï¼ˆ/ ã‚’å«ã‚€ï¼‰
      
    return new RegExp(`^${pattern}$`);
  }
}

// Parse CLI arguments
export function parseArgs(args: string[]): {
  url?: string;
  options?: CrawlOptions;
  format?: string;
  showHelp?: boolean;
  error?: string;
} {
  if (args.length === 0) {
    return { error: "URL is required" };
  }

  if (args.includes("--help") || args.includes("-h")) {
    return { showHelp: true };
  }

  let url: string | undefined;
  const options: CrawlOptions = {
    concurrency: DEFAULT_CONCURRENCY,
    match: [],
  };
  let format = FORMAT_TEXT;

  for (let i = 0; i < args.length; i++) {
    const arg = args[i];

    switch (arg) {
      case "-c":
      case "--concurrency":
        options.concurrency = parseInt(args[++i]);
        break;
      case "-m":
      case "--match":
        if (!options.match) options.match = [];
        options.match.push(args[++i]);
        break;
      case "-l":
      case "--limit":
        options.limit = parseInt(args[++i]);
        break;
      case "--json":
        format = FORMAT_JSON;
        break;
      case "--with-root":
        options.withRoot = true;
        break;
      default:
        // ãƒ€ãƒƒã‚·ãƒ¥ã§å§‹ã¾ã‚‰ãªã„å¼•æ•°ã§ã€ã¾ã URLãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
        if (!arg.startsWith("-") && !url) {
          url = arg;
        }
        break;
    }
  }

  if (!url) {
    return { error: "URL is required" };
  }

  return { url, options, format };
}

// CLI Interface
async function main() {
  const args = Deno.args;

  if (args.length === 0 || args.includes("--help") || args.includes("-h")) {
    console.log(`
URL Crawler - Extract links from websites

Usage:
  crawl.ts <url> [options]

Options:
  -c, --concurrency <n>    Number of concurrent requests (default: 3)
  -m, --match <pattern>    Only crawl URLs matching pattern (can be repeated)
  -l, --limit <n>          Maximum number of pages to crawl
  --no-same-host           Allow crawling external hosts
  --with-root              Include root and all paths (remove base path restriction)
  --json                   Output as JSON
  -h, --help               Show this help

Examples:
  crawl.ts https://example.com
  crawl.ts https://docs.example.com -m "/api/**" -m "/guide/**"
  crawl.ts https://example.com/docs/ --with-root -m "/api/**"
  crawl.ts https://example.com --limit 10 --json
    `.trim());
    Deno.exit(0);
  }

  const url = args.find((arg) => !arg.startsWith("-"))!;
  const options: CrawlOptions = {
    concurrency: 3,
    match: [],
    sameHost: !args.includes("--no-same-host"),
    withRoot: args.includes("--with-root"),
  };

  // Parse options
  for (let i = 0; i < args.length; i++) {
    const arg = args[i];

    switch (arg) {
      case "-c":
      case "--concurrency":
        options.concurrency = parseInt(args[++i]);
        break;
      case "-m":
      case "--match":
        options.match!.push(args[++i]);
        break;
      case "-l":
      case "--limit":
        options.limit = parseInt(args[++i]);
        break;
    }
  }

  // Crawl
  const crawler = new URLCrawler(url, options);
  const results = await crawler.crawl();

  // Output
  if (args.includes("--json")) {
    console.log(JSON.stringify(results, null, 2));
  } else {
    for (const result of results) {
      if (result.error) {
        console.error(`âŒ ${result.url}: ${result.error}`);
      } else {
        console.log(`\nğŸ“„ ${result.url}`);
        console.log(`   Found ${result.links.length} links`);
        if (result.links.length > 0 && result.links.length <= 10) {
          result.links.forEach((link) => console.log(`   â†’ ${link}`));
        }
      }
    }

    console.log(`\nâœ… Crawled ${results.length} pages total`);
  }
}

if (import.meta.main) {
  main();
}
