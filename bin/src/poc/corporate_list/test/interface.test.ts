/**
 * Step 2: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ãƒ†ã‚¹ãƒˆï¼ˆTDD RED Phaseï¼‰
 * 
 * ç›®çš„ï¼šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’äº¤æ›å¯èƒ½ã«ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®šç¾©
 * æ‰‹æ³•ï¼šBeckæµTDD - ã¾ãšå¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
 * 
 * @see bin/docs/conventions/tdd_process.md - ãƒ‘ã‚¿ãƒ¼ãƒ³A: æ–°è¦é–‹ç™ºTDDï¼ˆBeckæµï¼‰
 * @see bin/docs/conventions/dependency_management.md - é«˜éšŽé–¢æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³
 */

import { describe, expect, it } from 'bun:test'
import type { Browser } from 'playwright-core'
import { createPRTimesScraper, createDefaultPRTimesScraper } from '../packages/scraper-prtimes/src/mod.js'
import type { IScraper, ScrapedResult, BrowserConfig } from '../packages/scraper-core/src/mod.js'

/**
 * ä¾å­˜æ€§æ³¨å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé«˜éšŽé–¢æ•°ï¼‰
 * @see bin/docs/conventions/dependency_management.md - ãƒ«ãƒ¼ãƒ«: é«˜éšŽé–¢æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³
 */
const createScraperClient = (scraper: IScraper) => 
  async (browser: Browser, keywords: string[]): Promise<ScrapedResult[]> => {
    const results: ScrapedResult[] = []
    
    for (const keyword of keywords) {
      const keywordResults = await scraper.scrape(browser, keyword)
      results.push(...keywordResults)
    }
    
    return results
  }

describe('Step 2: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ï¼ˆTDD RED Phaseï¼‰', () => {
  
  it('ã€REDã€‘ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹', () => {
    // ã“ã®ãƒ†ã‚¹ãƒˆã¯REDãƒ•ã‚§ãƒ¼ã‚º - å®Ÿè£…å‰ãªã®ã§å¤±æ•—ã™ã‚‹
    const mockScraper: IScraper = {
      scrape: async (browser, keyword) => {
        return [{
          source: 'MOCK',
          company_name: 'ãƒ†ã‚¹ãƒˆæ ªå¼ä¼šç¤¾',
          title: `${keyword}ã®ãƒ†ã‚¹ãƒˆè¨˜äº‹`,
          url: 'https://example.com/test',
          scraped_at: new Date().toISOString()
        }]
      },
      getName: () => 'MockScraper'
    }
    
    expect(mockScraper.getName()).toBe('MockScraper')
    expect(typeof mockScraper.scrape).toBe('function')
  })

  it('ã€REDã€‘é«˜éšŽé–¢æ•°ã«ã‚ˆã‚‹ä¾å­˜æ€§æ³¨å…¥ãŒæ©Ÿèƒ½ã™ã‚‹', async () => {
    // ãƒ¢ãƒƒã‚¯ãƒ–ãƒ©ã‚¦ã‚¶ï¼ˆæœ€å°é™ã®å®Ÿè£…ï¼‰
    const mockBrowser = {} as Browser
    
    // ãƒ¢ãƒƒã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    const mockScraper: IScraper = {
      scrape: async (browser, keyword) => {
        return [{
          source: 'TEST',
          company_name: null,
          title: keyword,
          url: `https://test.com/${keyword}`,
          scraped_at: '2025-08-22T00:00:00.000Z'
        }]
      },
      getName: () => 'TestScraper'
    }
    
    // ä¾å­˜æ€§æ³¨å…¥
    const scraperClient = createScraperClient(mockScraper)
    
    // å®Ÿè¡Œ
    const results = await scraperClient(mockBrowser, ['test1', 'test2'])
    
    // æ¤œè¨¼
    expect(results).toHaveLength(2)
    expect(results[0].title).toBe('test1')
    expect(results[1].title).toBe('test2')
  })

  it('ã€REDã€‘è¤‡æ•°ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½', async () => {
    const mockBrowser = {} as Browser
    
    // ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼A
    const scraperA: IScraper = {
      scrape: async (browser, keyword) => [{
        source: 'SOURCE_A',
        company_name: 'Aç¤¾',
        title: `A: ${keyword}`,
        url: 'https://a.com',
        scraped_at: new Date().toISOString()
      }],
      getName: () => 'ScraperA'
    }
    
    // ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼B  
    const scraperB: IScraper = {
      scrape: async (browser, keyword) => [{
        source: 'SOURCE_B',
        company_name: 'Bç¤¾',
        title: `B: ${keyword}`,
        url: 'https://b.com',
        scraped_at: new Date().toISOString()
      }],
      getName: () => 'ScraperB'
    }
    
    // ä¾å­˜æ€§æ³¨å…¥ã§åˆ‡ã‚Šæ›¿ãˆ
    const clientA = createScraperClient(scraperA)
    const clientB = createScraperClient(scraperB)
    
    const resultsA = await clientA(mockBrowser, ['test'])
    const resultsB = await clientB(mockBrowser, ['test'])
    
    expect(resultsA[0].source).toBe('SOURCE_A')
    expect(resultsB[0].source).toBe('SOURCE_B')
  })

  it('ã€GREENã€‘ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…', () => {
    // ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°ï¼ˆé–¢æ•°åž‹ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    type ScraperFactory = {
      create: (type: 'prtimes' | 'mock') => IScraper
      list: () => string[]
    }
    
    const createScraperFactory = (config: BrowserConfig): ScraperFactory => {
      const scrapers = new Map<string, IScraper>()
      
      // PR Timesã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰
      scrapers.set('prtimes', createDefaultPRTimesScraper(config))
      
      // ãƒ¢ãƒƒã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
      scrapers.set('mock', {
        scrape: async (browser, keyword) => [{
          source: 'MOCK',
          company_name: null,
          title: keyword,
          url: 'https://mock.com',
          scraped_at: new Date().toISOString()
        }],
        getName: () => 'MockScraper'
      })
      
      return {
        create: (type) => {
          const scraper = scrapers.get(type)
          if (!scraper) {
            throw new Error(`Unknown scraper type: ${type}`)
          }
          return scraper
        },
        list: () => Array.from(scrapers.keys())
      }
    }
    
    const testConfig: BrowserConfig = {
      userAgent: 'test-agent',
      timeout: 10000,
      waitTime: 1000,
      launchArgs: []
    }
    
    const factory = createScraperFactory(testConfig)
    
    expect(factory.list()).toContain('prtimes')
    expect(factory.list()).toContain('mock')
    
    const mockScraper = factory.create('mock')
    expect(mockScraper.getName()).toBe('MockScraper')
    
    const prTimesScraper = factory.create('prtimes')
    expect(prTimesScraper.getName()).toBe('PRTimesScraper')
  })

  it('ã€REDã€‘ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹', async () => {
    const mockBrowser = {} as Browser
    
    // ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    const errorScraper: IScraper = {
      scrape: async (browser, keyword) => {
        if (keyword === 'error') {
          // ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ãšã«ç©ºé…åˆ—ã‚’è¿”ã™ï¼ˆè¦ç´„æº–æ‹ ï¼‰
          return []
        }
        return [{
          source: 'TEST',
          company_name: null,
          title: keyword,
          url: 'https://test.com',
          scraped_at: new Date().toISOString()
        }]
      },
      getName: () => 'ErrorHandlingScraper'
    }
    
    const client = createScraperClient(errorScraper)
    const results = await client(mockBrowser, ['success', 'error', 'success2'])
    
    // ã‚¨ãƒ©ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã¯çµæžœãŒ0ä»¶ã€ä»–ã¯æˆåŠŸ
    expect(results).toHaveLength(2)
    expect(results[0].title).toBe('success')
    expect(results[1].title).toBe('success2')
  })
})

describe('Step 2: æ—¢å­˜å®Ÿè£…ã¨ã®äº’æ›æ€§ç¢ºèª', () => {
  
  it('æ—¢å­˜ã®ScraperFactoryãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®æ•´åˆæ€§', () => {
    // æ—¢å­˜ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
    const existingPattern = {
      className: 'ScraperFactory',
      method: 'createPRTimesScraper',
      returns: 'BaseScraper instance'
    }
    
    // æ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã®äº’æ›æ€§ã‚’ç¢ºèª
    const newPattern = {
      function: 'createScraperFactory',
      method: 'create',
      returns: 'IScraper instance'
    }
    
    // ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå…±å­˜å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    expect(existingPattern.method).toContain('create')
    expect(newPattern.method).toBe('create')
  })

  it('æ—¢å­˜ã®åž‹å®šç¾©ã¨ã®äº’æ›æ€§', () => {
    // æ—¢å­˜ã®ScrapedResultåž‹ã¨åŒã˜æ§‹é€ 
    const result: ScrapedResult = {
      source: 'PR_TIMES',
      company_name: 'æ ªå¼ä¼šç¤¾Example',
      title: 'ã‚¿ã‚¤ãƒˆãƒ«',
      url: 'https://prtimes.jp/example',
      scraped_at: '2025-08-22T12:00:00.000Z'
    }
    
    // å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
    expect(result).toHaveProperty('source')
    expect(result).toHaveProperty('company_name')
    expect(result).toHaveProperty('title')
    expect(result).toHaveProperty('url')
    expect(result).toHaveProperty('scraped_at')
  })
})

/**
 * æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æº–å‚™
 * ã“ã‚Œã‚‰ã®ãƒ†ã‚¹ãƒˆãŒé€šã£ãŸã‚‰ã€Step 3ã§å®Ÿéš›ã®å®Ÿè£…ã‚’è¡Œã†
 */
describe('Step 2: GREEN Phaseã¸ã®æº–å‚™', () => {
  
  it('å®Ÿè£…ã™ã¹ãã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä»•æ§˜ã‚’æ˜Žç¢ºåŒ–', () => {
    const specifications = {
      interface: 'IScraper',
      methods: ['scrape', 'getName'],
      dependencyInjection: 'High-order function pattern',
      errorHandling: 'Return empty array instead of throwing',
      compatibility: 'Maintain existing ScrapedResult type'
    }
    
    console.log('\nðŸ“‹ === INTERFACE SPECIFICATIONS ===')
    console.log('Interface:', specifications.interface)
    console.log('Methods:', specifications.methods.join(', '))
    console.log('DI Pattern:', specifications.dependencyInjection)
    console.log('Error Handling:', specifications.errorHandling)
    console.log('Compatibility:', specifications.compatibility)
    console.log('=====================================\n')
    
    expect(specifications.methods).toHaveLength(2)
  })
})