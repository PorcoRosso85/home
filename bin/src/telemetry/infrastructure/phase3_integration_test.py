"""
Phase 3 integration test

è²¬å‹™: Phase 3ã®å…¨æ©Ÿèƒ½ã®çµ±åˆå‹•ä½œç¢ºèª
- DuckDBå‹•ä½œ
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ©Ÿèƒ½
- ãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½
"""

from telemetry.infrastructure.persistence.duckdbRepository import create_duckdb_telemetry_repository
from telemetry.infrastructure.formatters.telemetryFormatter import (
    format_json, format_human_readable, format_compact, format_csv_header, format_csv_row,
    create_formatter
)
from telemetry.infrastructure.parsers.telemetryParser import (
    parse_json, parse_dict, parse_claude_stream, parse_syslog, parse_generic_log,
    create_parser
)
from telemetry.application.capture.streamCapture import create_stream_capture
import tempfile
import os


def test_phase3_duckdb_basic_operations():
    """DuckDBã®åŸºæœ¬æ“ä½œ"""
    repo = create_duckdb_telemetry_repository(":memory:")
    
    # ä¿å­˜
    log_record = {
        "type": "log",
        "timestamp": "2024-01-01T00:00:00Z",
        "body": "DuckDB test",
        "severity": "INFO"
    }
    
    result = repo.save(log_record)
    assert "id" in result
    assert "error" not in result
    
    # ã‚¯ã‚¨ãƒª
    logs = repo.query_by_type("log")
    assert len(logs) == 1
    assert logs[0]["body"] == "DuckDB test"


def test_phase3_duckdb_json_capabilities():
    """DuckDBã®JSONæ©Ÿèƒ½æ´»ç”¨"""
    repo = create_duckdb_telemetry_repository(":memory:")
    
    # è¤‡é›‘ãªJSONãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ¬ã‚³ãƒ¼ãƒ‰
    complex_record = {
        "type": "span",
        "timestamp": "2024-01-01T00:00:00Z",
        "span_id": "span-123",
        "trace_id": "trace-456",
        "name": "database.query",
        "attributes": {
            "db.type": "postgresql",
            "db.statement": "SELECT * FROM users",
            "db.rows_affected": 42
        },
        "events": [
            {"timestamp": "2024-01-01T00:00:00.100Z", "name": "query.start"},
            {"timestamp": "2024-01-01T00:00:00.200Z", "name": "query.end"}
        ]
    }
    
    repo.save(complex_record)
    spans = repo.query_by_type("span")
    
    assert len(spans) == 1
    saved_span = spans[0]
    assert saved_span["attributes"]["db.type"] == "postgresql"
    assert len(saved_span["events"]) == 2


def test_phase3_formatters_all_types():
    """å…¨ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    record = {
        "type": "metric",
        "timestamp": "2024-01-01T00:00:00Z",
        "name": "cpu.usage",
        "value": 75.5,
        "unit": "%"
    }
    
    # JSONå½¢å¼
    json_str = format_json(record)
    assert "cpu.usage" in json_str
    
    # äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼
    human_str = format_human_readable(record)
    assert "Type: metric" in human_str
    assert "Value: 75.5" in human_str
    
    # ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå½¢å¼
    compact_str = format_compact(record)
    assert "[2024-01-01T00:00:00Z] METRIC: cpu.usage=75.5%" == compact_str
    
    # CSVå½¢å¼
    csv_row = format_csv_row(record)
    fields = csv_row.split(",")
    assert fields[2] == "cpu.usage"  # name
    assert fields[3] == "75.5"  # value
    assert fields[9] == "%"  # unit


def test_phase3_parsers_all_types():
    """å…¨ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # JSON ãƒ‘ãƒ¼ã‚µãƒ¼
    json_str = '{"type": "log", "body": "JSON log", "severity": "WARN"}'
    result = parse_json(json_str)
    assert "record" in result
    assert result["record"]["severity"] == "WARN"
    
    # Claude stream ãƒ‘ãƒ¼ã‚µãƒ¼
    claude_str = '{"event": "content_block_delta", "delta": {"text": "Claude says hello"}}'
    result = parse_claude_stream(claude_str)
    assert "record" in result
    assert result["record"]["body"] == "Claude says hello"
    
    # Syslog ãƒ‘ãƒ¼ã‚µãƒ¼
    syslog_str = "Jan  1 12:00:00 server nginx[1234]: Request processed"
    result = parse_syslog(syslog_str)
    assert "record" in result
    assert result["record"]["resource"]["process"] == "nginx"
    
    # æ±ç”¨ãƒ­ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼
    generic_str = "2024-01-01T12:00:00Z [INFO] Application started"
    result = parse_generic_log(generic_str)
    assert "record" in result
    assert result["record"]["severity"] == "INFO"


def test_phase3_parse_format_roundtrip():
    """ãƒ‘ãƒ¼ã‚¹â†’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—"""
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿
    original_json = '{"type":"log","timestamp":"2024-01-01T00:00:00Z","body":"Test message","severity":"ERROR"}'
    
    # ãƒ‘ãƒ¼ã‚¹
    parse_result = parse_json(original_json)
    assert "record" in parse_result
    record = parse_result["record"]
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_json = format_json(record)
    
    # å†åº¦ãƒ‘ãƒ¼ã‚¹
    reparse_result = parse_json(formatted_json)
    assert "record" in reparse_result
    reparsed = reparse_result["record"]
    
    # åŒã˜å†…å®¹ã‹ç¢ºèª
    assert reparsed["type"] == record["type"]
    assert reparsed["body"] == record["body"]
    assert reparsed["severity"] == record["severity"]


def test_phase3_duckdb_with_stream_capture():
    """DuckDBã¨ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚­ãƒ£ãƒ—ãƒãƒ£ã®çµ±åˆ"""
    repo = create_duckdb_telemetry_repository(":memory:")
    capture_fn = create_stream_capture(repo)
    
    # æ§˜ã€…ãªå½¢å¼ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ 
    mixed_stream = [
        '{"type": "log", "body": "Start processing"}',
        '{"type": "metric", "name": "items.processed", "value": 0}',
        '{"event": "content_block_delta", "delta": {"text": "Processing item 1"}}',
        '{"type": "metric", "name": "items.processed", "value": 1}',
        '{"type": "log", "body": "Processing complete", "severity": "INFO"}'
    ]
    
    result = capture_fn(iter(mixed_stream))
    assert result["processed"] == 5
    assert result["errors"] == 0
    
    # ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    total_count = repo.count()
    assert total_count == 5
    
    metrics = repo.query_by_type("metric")
    assert len(metrics) == 2


def test_phase3_formatter_parser_integration():
    """ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã¨ãƒ‘ãƒ¼ã‚µãƒ¼ã®çµ±åˆ"""
    # ãƒ‘ãƒ¼ã‚µãƒ¼ä½œæˆ
    json_parser = create_parser("json")
    claude_parser = create_parser("claude")
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ä½œæˆ
    json_formatter = create_formatter("json")
    human_formatter = create_formatter("human")
    
    # Claude streamã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    claude_data = '{"event": "message_start", "message": {"id": "msg_123"}}'
    parse_result = claude_parser(claude_data)
    
    if "record" in parse_result:
        record = parse_result["record"]
        
        # JSONå½¢å¼ã§å‡ºåŠ›
        json_output = json_formatter(record)
        assert isinstance(json_output, str)
        
        # äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
        human_output = human_formatter(record)
        assert "Type: log" in human_output


def test_phase3_csv_export_workflow():
    """CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
    repo = create_duckdb_telemetry_repository(":memory:")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
    test_data = [
        {"type": "log", "timestamp": "2024-01-01T00:00:00Z", "body": "App start", "severity": "INFO"},
        {"type": "metric", "timestamp": "2024-01-01T00:00:01Z", "name": "memory.used", "value": 512, "unit": "MB"},
        {"type": "span", "timestamp": "2024-01-01T00:00:02Z", "span_id": "s1", "trace_id": "t1", "name": "http.request", "duration": 150}
    ]
    
    repo.save_batch(test_data)
    
    # CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    csv_lines = [format_csv_header()]
    
    # å„ã‚¿ã‚¤ãƒ—ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦CSVåŒ–
    for record_type in ["log", "metric", "span"]:
        records = repo.query_by_type(record_type)
        for record in records:
            csv_lines.append(format_csv_row(record))
    
    # CSVå‡ºåŠ›ã®ç¢ºèª
    assert len(csv_lines) == 4  # header + 3 records
    assert "timestamp,type,name" in csv_lines[0]


def test_phase3_duckdb_persistence():
    """DuckDBãƒ•ã‚¡ã‚¤ãƒ«æ°¸ç¶šæ€§"""
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã ã‘ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã—ãªã„ï¼‰
    fd, db_path = tempfile.mkstemp(suffix=".duckdb")
    os.close(fd)
    os.unlink(db_path)  # ä¸€æ—¦å‰Šé™¤ã—ã¦DuckDBã«ä½œæˆã•ã›ã‚‹
    
    try:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³1: ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        repo1 = create_duckdb_telemetry_repository(db_path)
        repo1.save({
            "type": "metric",
            "timestamp": "2024-01-01T00:00:00Z",
            "name": "test.metric",
            "value": 42.0
        })
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³2: ãƒ‡ãƒ¼ã‚¿å–å¾—
        repo2 = create_duckdb_telemetry_repository(db_path)
        metrics = repo2.query_by_type("metric")
        
        assert len(metrics) == 1
        assert metrics[0]["value"] == 42.0
    finally:
        os.unlink(db_path)


def run_phase3_verification():
    """Phase 3ã®å®Œå…¨ãªå‹•ä½œç¢ºèª"""
    print("Phase 3 å‹•ä½œç¢ºèªé–‹å§‹...")
    
    tests = [
        ("DuckDBåŸºæœ¬æ“ä½œ", test_phase3_duckdb_basic_operations),
        ("DuckDB JSONæ©Ÿèƒ½", test_phase3_duckdb_json_capabilities),
        ("å…¨ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼", test_phase3_formatters_all_types),
        ("å…¨ãƒ‘ãƒ¼ã‚µãƒ¼", test_phase3_parsers_all_types),
        ("ãƒ‘ãƒ¼ã‚¹ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¾€å¾©", test_phase3_parse_format_roundtrip),
        ("DuckDBï¼‹ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚­ãƒ£ãƒ—ãƒãƒ£", test_phase3_duckdb_with_stream_capture),
        ("ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ãƒ»ãƒ‘ãƒ¼ã‚µãƒ¼çµ±åˆ", test_phase3_formatter_parser_integration),
        ("CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", test_phase3_csv_export_workflow),
        ("DuckDBæ°¸ç¶šæ€§", test_phase3_duckdb_persistence)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            test_func()
            print(f"âœ… {test_name}")
            passed += 1
        except AssertionError as e:
            print(f"âŒ {test_name}: {e}")
            failed += 1
        except Exception as e:
            print(f"ğŸ’¥ {test_name}: {type(e).__name__}: {e}")
            failed += 1
    
    print(f"\nçµæœ: {passed} æˆåŠŸ, {failed} å¤±æ•—")
    return failed == 0


if __name__ == "__main__":
    success = run_phase3_verification()
    exit(0 if success else 1)