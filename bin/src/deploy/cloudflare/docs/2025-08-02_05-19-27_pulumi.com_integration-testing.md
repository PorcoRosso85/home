---
url: https://www.pulumi.com/docs/iac/concepts/testing/integration/
saved_at: 2025-08-02T05:19:27Z
title: Integration testing for Pulumi programs
domain: pulumi.com
---

1. [Docs](/docs/)
1. [Pulumi IaC](/docs/iac/)
1. [Concepts](/docs/iac/concepts/)
1. [Testing](/docs/iac/concepts/testing/)
1. [Integration testing](/docs/iac/concepts/testing/integration/)

# Integration testing for Pulumi programs

## On this page

## On this page

- [** Edit this Page](https://github.com/pulumi/docs/edit/master/content/docs/iac/concepts/testing/integration.md)
- [** Request a Change](https://github.com/pulumi/docs/issues/new?body=File: [content/docs%2fiac%2fconcepts%2ftesting%2fintegration.md](https%3a%2f%2fwww.pulumi.com%2fdocs%2fiac%2fconcepts%2ftesting%2fintegration%2f))

Integration testing focuses on black-box testing of Pulumi programs. An integration test runs the program in combination with the Pulumi CLI to deploy infrastructure to an ephemeral environment. It verifies the properties of the created resources and then destroys the infrastructure again.

By running a program through integration tests, you can ensure:

- Your project ' s code is syntactically well-formed and runs without errors.
- Your stack ' s configuration and secrets work and are interpreted correctly.
- Your project can be successfully deployed to your cloud provider of choice.
- Resources of the desired shape are successfully created.
- The infrastructure behaves as expected: for example, a health-check endpoint returns a valid HTML document, or a suite of application-level tests succeeds against the public API.
- Your project can be successfully updated from its starting state to other states.
- Your project can be successfully destroyed and removed from your cloud provider.

In principle, integration tests can be written in any general-purpose programming language. Tests do not interact with the program-under-test directly: instead, they utilize Pulumi to create, update, and delete cloud infrastructure.

## Pulumi ' s Integration Test Framework

At Pulumi, we maintain an extensive suite of integration tests to validate the functionality of the core CLI and providers. To facilitate this testing, Pulumi has an integration test framework written in Go.

This framework has been built to take a directory containing a full Pulumi program and drive various lifecycle operations against it: deploying a new stack from scratch, updating it with variations, and tearing it down afterwards, potentially multiple times. We run these tests for each pull request, regularly (such as nightly), and as stress tests.

Below, we provide walk through leveraging the Pulumi integration test framework. You can use the Go test framework no matter the language your Pulumi program is written in. At the end of this guide, we discuss using Pulumi ' s Automation API, which is available in all Pulumi-supported languages, as an alternative to the integration test framework.

### A Basic Integration Test

The following program is a simplified test of our example that provisions an [S3 bucket and objects in Pulumi](https://github.com/pulumi/examples/tree/master/aws-go-s3-folder):

```go
package test

import (
    "os"
    "path"
    "testing"

    "github.com/pulumi/pulumi/pkg/v2/testing/integration"
    "github.com/stretchr/testify/assert"
)

func TestAccS3Folder(t *testing.T) {
    cwd, _ := os.Getwd()
    test := integration.ProgramTestOptions{
        Dir: path.Join(cwd, ".", ".", "aws-go-s3-folder"),
        SkipRefresh: true,
        ExtraRuntimeValidation: func(t *testing.T, info integration.RuntimeValidationStackInfo) {
            // Simple runtime validation that just ensures the stack has an output called "websiteUrl".
            assert.NotNil(t, info.Outputs["websiteUrl"])
        },
    }
    integration.ProgramTest(t, &test)
}
```

This test does several things:

1. By default, `ProgramTest` will create a temporary directory and copy the Go program to it. `Dir` indicates which directory to copy.
2. It will create a new stack using `pulumi stack init` with a random name.
3. It will run `pulumi up` and wait for it to successfully complete.
4. It will run extraRuntimeValidation when your project is in the state described by the Pulumi program.
5. It will tear down the stack by running `pulumi destroy --yes`.

The extraRuntimeValidation function gives you a chance to validate the shape of the resources that have been created. Outputs will contain a map of stack output names to the value of that output: in our case, we just make sure that websiteUrl is not nil. You can also use HTTP libraries to test properties of the deployed stack by actually making requests.

### Validating the shape of resources

Integration test frameworks often allow property-bag diffing: given the property bag of a resource and an expected property bag, a diff is conducted and a test failure generated if there is a discrepancy.

This approach has drawbacks:

- stack exports are extremely verbose when they contain all properties.
- it ' s difficult to write diffs that are useful.
- often, exports need to be hand-edited when the resulting infrastructure changes in minor ways (such as an S3 bucket being provisioned in a different AWS region).

Instead, the integration test framework encourages you to take a different approach: ensure your Pulumi program exports the data you need to validate, and then write pointed unit tests in the validation logic. For instance, if you were testing a simple serverless REST API (such as the example from the [cloud-aws-apigateway-quickstart example](https://github.com/pulumi/examples/tree/master/cloud-js-api), you could check to ensure the API Gateway object contained the correct number of routes:

```javascript
"use strict";
let cloud = require("@pulumi/cloud-aws");

let api = new cloud.API("my-api");
api.get("/hello", (req, res) => {
    res.json({ hello: world });
});

// Add a route to count the number of times 'route' has been hit.
let helloCount = 0;
let counterTable = new cloud.Table("counterTable", "route");

api.get("/count", async (req, res) => {
    helloCount++;
    let result = await counterTable.get(route);
    res.json({ hello: hello, count: helloCount });
});

exports.url = api.publish().url;
exports.routes = [ "/hello", "/count" ];
```

The test would resemble:

```go
ExtraRuntimeValidation: func(t *testing.T, info integration.RuntimeValidationStackInfo) {
    var routes []string
    assert.NoError(t, json.Unmarshal(info.Outputs["routes"], &routes))
    assert.Equal(t, 2, len(routes))
},
```

### Complex initialization

In the common case, integration tests just validate a single Pulumi program. However, you may have scenarios where your Pulumi program deploys into an existing environment that has already been provisioned by another program. Or, your program may reference resources in another stack that must exist before your program can run. Scenarios like these require more complex initialization.

Let ' s say your application requires a VPC and RDS database to be provisioned before it is deployed. The integration test would resemble:

```go
test := integration.ProgramTestOptions{
    // Where the VPC Pulumi program is.
    Dir: "/path/to/vpc/infrastructure",
    StackName: vpcStackName,
    // Set up the VPC.
    EditDirs: []integration.EditDir{
        {
            // Where the RDS Pulumi program is.
            Dir:      "/path/to/rds/infrastructure",
            Additive: true,
            ExtraRuntimeValidation: func(
                t *testing.T,
                stack integration.RuntimeValidationStackInfo) {
                // Validate the RDS configuration.
            },
        },
        {
            // Where the application Pulumi program is.
            Dir:      "/path/to/app/infrastructure",
            Additive: true,
            ExtraRuntimeValidation: func(
                t *testing.T,
                stack integration.RuntimeValidationStackInfo) {
                // Validate the application configuration.
            },
        },
    },
}
```

Here, we ' re taking advantage of the integration test ' s notion of "editing" the source program and rerunning `pulumi up`. We have marked these edits as `Additive: true` : this indicates the Pulumi programs in `Dir` are an addition to the existing program, as opposed to a replacement.

The result is a complex test that provisions a VPC, RDS instance, and application infrastructure that takes advantage of the two former resources.

### Other Go integration test framework features

You ' ve already seen ways to drive integration tests that deploy multiple Pulumi programs. There are many other options at your disposal:

- You can run Pulumi programs against multiple cloud providers by passing more than one `Cloud` in `integration.ProgramTestOptions`. This is useful if you want to ensure that a code change works against AWS, Azure, Kubernetes, and other clouds simultaneously.
- You can leave infrastructure up on test failures: pass `NoDestroy: true` inside `integration.ProgramTestOptions`. All subsequent test runs will use the stack rather than create a new one. A destroy phase will happen during the first successful test run, removing this infrastructure. This is useful for debugging failing tests, especially in CI scenarios where you want to "plug into" the infrastructure.
- You can use an existing stack in your tests by passing `StackName` inside `integration.ProgramTestOptions`. This is useful if you want to use a stack with existing configuration and secrets.
- You can test structured configuration by specifying JSON in the `Config` property, and secrets work by populating the `Secrets` map.
- You can have a Pulumi program specifically created for testing, rather than an actual example or infrastructure project. If you set `PrepareProject` to a function inside `integration.ProgramTestOptions`, it will be invoked and can install dependencies.
- You can skip installing dependencies if a `Pulumi.yaml` inside the program indicates the dependencies have already been installed. Set `Quick: true` inside `integration.ProgramTestOptions` to enable this option.
- You can include and exclude tests by filtering by tag. If you run tests with the `-run-tags` argument, a comma-separated list of tags can be specified. If the `integration.ProgramTestOptions` contains one or more of these tags in its `CloudTags` property, the test will run.

## Frequently Asked Questions

### How can I validate that a resource has certain properties?

Given a stack ' s JSON export, you can add code to the `ExtraRuntimeValidation` handler to look for specific resources to make assertions about. Here ' s an example in Go:

```go
ExtraRuntimeValidation: func(t *testing.T, stack integration.RuntimeValidationStackInfo) {
    // Export the raw resources in the stack._sectionLayout
    bytes, err := json.Marshal(stack.Deployment)
    assert.NoError(t, err)
    t.Log(string(bytes))

    // Iterate specifically azure:core/resourceGroup:ResourceGroup types.
    for _, res := range stack.Deployment.Resources {
        if res.Type == "azure:core/resourceGroup:ResourceGroup" {
            // Check that the location field was set to WestUS.
            assert.Equal(t, "westus", res.Outputs["location"])
        }
    }
},
```

### Can I use integration tests with any language supported by Pulumi, or just Go?

The test framework is written in Go, but the `Dir` folder can be a Pulumi program in any language. Pulumi will automatically detect the language and execute the correct runtime. This means that even if you write your tests in Go, you can still test Node.js, Python and .NET programs.

### What about unit tests?

The integration test framework is only for black-box integration testing. Multi-language unit testing is supported. Read more on the [unit testing guide](/docs/iac/concepts/testing/unit).

### How can I write integration tests in other languages?

Pulumi ' s Automation API is available in all Pulumi-supported languages and provides a natural way to write integration tests for Pulumi programs by driving the Pulumi CLI programmatically. See the Automation API guide for code examples, including samples for [testing](https://www.pulumi.com/docs/iac/packages-and-automation/automation-api/#running-tests).