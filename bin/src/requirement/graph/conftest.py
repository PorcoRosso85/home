"""
å…±é€šã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
é«˜é€Ÿãªã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã‚’æä¾›
"""
import pytest
import tempfile
import subprocess
import json
import sys
import os
from typing import Dict, Any, Optional
from pathlib import Path

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ç”¨
from test_utils.performance import PerformanceCollector, measure_time


def run_system_optimized(input_data: Dict[str, Any], db_path: Optional[str] = None, timeout: int = 30) -> Dict[str, Any]:
    """æœ€é©åŒ–ã•ã‚ŒãŸrun_systemé–¢æ•°ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä»˜ãï¼‰"""
    env = os.environ.copy()
    if db_path:
        env["RGL_DATABASE_PATH"] = db_path

    python_cmd = sys.executable
    
    # ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆ
    process = subprocess.Popen(
        [python_cmd, "-m", "requirement.graph"],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env
    )
    
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
        stdout, stderr = process.communicate(
            input=json.dumps(input_data),
            timeout=timeout
        )
    except subprocess.TimeoutExpired:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯å¼·åˆ¶çµ‚äº†
        process.kill()
        stdout, stderr = process.communicate()
        return {
            "error": "Process timeout",
            "stderr": stderr,
            "timeout": timeout
        }
    finally:
        # ç¢ºå®Ÿã«ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if process.poll() is None:
            process.terminate()
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()
    
    # çµæœã‚’ãƒ‘ãƒ¼ã‚¹
    if stdout:
        lines = stdout.strip().split('\n')
        for line in reversed(lines):
            if line.strip():
                try:
                    return json.loads(line)
                except json.JSONDecodeError:
                    continue
    
    return {"error": "No valid JSON output", "stderr": stderr}


@pytest.fixture
def inmemory_db():
    """ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    # KuzuDBã¯:memory:ã ã‘ã§ç‹¬ç«‹ã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    db_path = ":memory:"
    
    # ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–
    result = run_system_optimized({"type": "schema", "action": "apply"}, db_path)
    if "error" in result:
        pytest.fail(f"Failed to initialize schema: {result}")
    
    yield db_path
    
    # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBã¯è‡ªå‹•çš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹


@pytest.fixture
def file_db():
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰"""
    with tempfile.TemporaryDirectory() as db_dir:
        # ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–
        result = run_system_optimized({"type": "schema", "action": "apply"}, db_dir)
        if "error" in result:
            pytest.fail(f"Failed to initialize schema: {result}")
        
        yield db_dir


@pytest.fixture
def run_system():
    """æœ€é©åŒ–ã•ã‚ŒãŸrun_systemé–¢æ•°ã‚’æä¾›ã™ã‚‹ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    return run_system_optimized


# E2Eãƒ†ã‚¹ãƒˆã®ãƒãƒ¼ã‚«ãƒ¼ã‚’è‡ªå‹•çš„ã«è¿½åŠ 
import inspect

def pytest_collection_modifyitems(items):
    """run_systemã‚’ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¹ãƒˆã«è‡ªå‹•çš„ã«e2eãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ """
    for item in items:
        # ãƒ†ã‚¹ãƒˆé–¢æ•°ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
        if hasattr(item, "function"):
            try:
                source = inspect.getsource(item.function)
                if "run_system" in source:
                    item.add_marker(pytest.mark.e2e)
            except (OSError, TypeError):
                # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                pass


# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
@pytest.fixture(scope="session")
def perf_collector():
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
    
    ä½¿ç”¨ä¾‹:
        def test_slow_operation(perf_collector):
            with perf_collector.measure("database_query", threshold=1.0):
                result = expensive_operation()
    """
    collector = PerformanceCollector("test_session")
    yield collector
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    report_path = Path("test_performance_report.json")
    report = collector.report(report_path)
    print(f"\nPerformance Report saved to: {report_path}")
    print(f"Total measurements: {report['total_measurements']}")
    
    # é…ã„ãƒ†ã‚¹ãƒˆã®è­¦å‘Š
    slow_tests = []
    for name, stats in report["statistics"].items():
        if stats["max"] > 5.0:  # 5ç§’ä»¥ä¸Š
            slow_tests.append((name, stats["max"]))
    
    if slow_tests:
        print("\nâš ï¸  Slow tests detected:")
        for name, duration in sorted(slow_tests, key=lambda x: x[1], reverse=True):
            print(f"  - {name}: {duration:.2f}s")


# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œæ™‚é–“è¡¨ç¤ºï¼ˆç’°å¢ƒå¤‰æ•°ã§æœ‰åŠ¹åŒ–ï¼‰
import time
from datetime import datetime

if os.environ.get("PYTEST_REALTIME", ""):
    test_start_times = {}
    
    @pytest.hookimpl(tryfirst=True)
    def pytest_runtest_setup(item):
        """å„ãƒ†ã‚¹ãƒˆã®é–‹å§‹æ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        test_start_times[item.nodeid] = time.time()
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ Starting: {item.nodeid}")
    
    @pytest.hookimpl(trylast=True)
    def pytest_runtest_teardown(item, nextitem):
        """å„ãƒ†ã‚¹ãƒˆã®çµ‚äº†æ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        if item.nodeid in test_start_times:
            duration = time.time() - test_start_times[item.nodeid]
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… Finished: {item.nodeid} ({duration:.2f}s)")
            del test_start_times[item.nodeid]


@pytest.fixture
def measure():
    """
    å€‹åˆ¥ãƒ†ã‚¹ãƒˆç”¨ã®è¨ˆæ¸¬ãƒ˜ãƒ«ãƒ‘ãƒ¼
    
    ä½¿ç”¨ä¾‹:
        def test_something(measure):
            with measure("setup", threshold=0.5):
                prepare_data()
                
            with measure("main_operation", threshold=2.0):
                result = process_data()
    """
    collector = PerformanceCollector("single_test")
    
    def _measure(name: str, threshold: Optional[float] = None, **metadata):
        return collector.measure(name, threshold, **metadata)
    
    yield _measure
    
    # ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã«çµ±è¨ˆã‚’å‡ºåŠ›ï¼ˆverboseæ™‚ã®ã¿ï¼‰
    if collector.measurements:
        print(f"\nTest performance summary:")
        for name, stats in collector.get_statistics().items():
            print(f"  {name}: {stats['mean']:.3f}s (n={stats['count']})")